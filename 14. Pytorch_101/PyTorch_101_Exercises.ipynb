{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gQXaOBBo1oW"
   },
   "source": [
    "# PyTorch 101\n",
    "\n",
    "Create a conda environment:\n",
    "\n",
    "`conda create -n pytorch101 pip conda python==3.6.8`\n",
    "\n",
    "Install PyTorch: https://pytorch.org/get-started/locally/\n",
    "Install GraphViz: https://www.graphviz.org/download/\n",
    "\n",
    "Install additionak packages:\n",
    "\n",
    "`conda install scikit-learn==0.21.3 matplotlib==3.1.1 jupyter -c anaconda`\n",
    "\n",
    "`pip install torchviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEp40UvmAmPK"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyE2ltG3Auin"
   },
   "source": [
    "**PyTorch** is the **fastest growing** Deep Learning framework and it is also used by **Fast.ai** in its MOOC, [Deep Learning for Coders](https://course.fast.ai/) and its [library](https://docs.fast.ai/).\n",
    "\n",
    "PyTorch is also very *pythonic*, meaning, it feels more natural to use it if you already are a Python developer.\n",
    "\n",
    "Besides, using PyTorch may even improve your health, according to [Andrej Karpathy](https://twitter.com/karpathy/status/868178954032513024) :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSt6oP0rA_FG"
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPZG7RmuBCv0"
   },
   "source": [
    "There are *many many* PyTorch tutorials around and its documentation is quite complete and extensive. So, **why** should you keep reading this step-by-step tutorial?\n",
    "\n",
    "Well, even though one can find information on pretty much anything PyTorch can do, I missed having a **structured, incremental and from first principles** approach to it.\n",
    "\n",
    "In this tutorial, I will guide you through the *main reasons* why PyTorch makes it much **easier** and more **intuitive** to build a Deep Learning model in Python — **autograd, dynamic computation graph, model classes** and more — and I will also show you **how to avoid some common pitfalls and errors** along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Y3FcRg0qJ0s"
   },
   "source": [
    "## A Simple Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlYSOAy8smmJ"
   },
   "source": [
    "Most tutorials start with some nice and pretty *image classification problem* to illustrate how to use PyTorch. It may seem cool, but I believe it **distracts** you from the **main goal: how PyTorch works**?\n",
    "\n",
    "For this reason, in this tutorial, I will stick with a **simple** and **familiar** problem: a **linear regression with a single feature x**! It doesn’t get much simpler than that…\n",
    "\n",
    "$$\n",
    "\\large y = a + b x + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHiXYWCMrGEr"
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7gu_5iGsVrA"
   },
   "source": [
    "Let’s start **generating** some synthetic data: we start with a vector of 100 points for our **feature x** and create our **labels** using **a = 1, b = 2** and some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-MRtiMeAsos"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l36meXzoop0U"
   },
   "outputs": [],
   "source": [
    "true_a = 1\n",
    "true_b = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "y = true_a + true_b * x + .1 * np.random.randn(N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGWRIovYrsxp"
   },
   "source": [
    "### Train / Validation Split\n",
    "\n",
    "Next, let’s **split** our synthetic data into **train** and **validation** sets, shuffling the array of indices and using the first 80 shuffled points for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6itdbvQXp8Xx"
   },
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "D4E8Dqaar1bn",
    "outputId": "65890dd4-6db3-4fa2-f2cd-cf872f57942b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].scatter(x_train, y_train)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_ylim([1, 3])\n",
    "ax[0].set_title('Generated Data - Train')\n",
    "ax[1].scatter(x_val, y_val, c='r')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_ylim([1, 3])\n",
    "ax[1].set_title('Generated Data - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJXRp3A-sPRG"
   },
   "source": [
    "We **know** that a = 1 and b = 2, but now let’s see how close we can get to the true values by using **gradient descent** and the 80 points in the **training set**…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2SePFR0syOS"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aI0IWVbPtGzw"
   },
   "source": [
    "It goes beyond the scope of this tutorial to fully explain how gradient descent works, but I'll cover the **five basic steps** you'd need to go through to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R04jMPtj2wtg"
   },
   "source": [
    "### Step 0: Initialization\n",
    "\n",
    "Technically, this step is not part of gradient descent, but it is an important step nonetheless.\n",
    "\n",
    "For training a model, you need to **randomly initialize the parameters/weights** (we have only two, **a** and **b**).\n",
    "\n",
    "Make sure to *always initialize your random seed* to ensure **reproducibility** of your results. As usual, the random seed is [42](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)), the *least random* of all random seeds one could possibly choose :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nMIZZu9o4JNs",
    "outputId": "8ff26624-e04f-4465-a405-e55e12e2ebcf"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TO1oNfOq5_Oi"
   },
   "source": [
    "### Step 1: Compute Model's Predictions\n",
    "\n",
    "This is the **forward pass** - it simply *computes the model's predictions using the current values of the parameters/weights*. At the very beginning, we will be producing really bad predictions, as we started with random values from Step 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAIfJb-v6eo0"
   },
   "outputs": [],
   "source": [
    "# Computes our model's predicted output\n",
    "yhat = a + b * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAJChW8Gs2vB"
   },
   "source": [
    "### Step 2: Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFkXT7untW14"
   },
   "source": [
    "There is a subtle but fundamental difference between **error** and **loss**. \n",
    "\n",
    "The **error** is the difference between **actual** and **predicted** computed for a single data point.\n",
    "\n",
    "The **loss**, on the other hand, is some sort of **aggregation of errors for a set of data points**.\n",
    "\n",
    "For a regression problem, the **loss** is given by the **Mean Square Error (MSE)**, that is, the average of all squared differences between **actual values** (y) and **predictions** (a + bx).\n",
    "\n",
    "---\n",
    "\n",
    "*It is worth mentioning that, if we use **all points** in the training set (N) to compute the loss, we are performing a **batch** gradient descent. If we were to use a **single point** at each time, it would be a **stochastic** gradient descent. Anything else (n) **in-between 1 and N** characterizes a **mini-batch** gradient descent.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBlHhFvHzujt"
   },
   "source": [
    "**Loss**:\n",
    "\n",
    "$$\n",
    "\\large MSE = \\frac{1}{N} \\sum_{i=1}^N{(y_i - \\hat{y_i})}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large MSE = \\frac{1}{N} \\sum_{i=1}^N{(y_i - a - b x_i)}^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vF-utTp22es0",
    "outputId": "52bf9311-a17f-484c-ca73-5da7c7d8e750"
   },
   "outputs": [],
   "source": [
    "# How wrong is our model? That's the error! \n",
    "error = (y_train - yhat)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bO8K6rePs48M"
   },
   "source": [
    "### Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-RZUTYvzDGB"
   },
   "source": [
    "A **gradient** is a **partial derivative** — *why partial*? Because one computes it with respect to (w.r.t.) a **single parameter**. We have two parameters, **a** and **b**, so we must compute two partial derivatives.\n",
    "\n",
    "A **derivative** tells you *how much* **a given quantity changes** when you *slightly* vary some **other quantity**. In our case, how much does our **MSE** **loss** change when we vary **each one of our two parameters**?\n",
    "\n",
    "The *right-most* part of the equations below is what you usually see in implementations of gradient descent for a simple linear regression. In the **intermediate step**, I show you **all elements** that pop-up from the application of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), so you know how the final expression came to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaSuPRY5zxqb"
   },
   "source": [
    "**Gradients**:\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial{MSE}}{\\partial{a}} = \\frac{\\partial{MSE}}{\\partial{\\hat{y_i}}} \\cdot \\frac{\\partial{\\hat{y_i}}}{\\partial{a}} = \\frac{1}{N} \\sum_{i=1}^N{2(y_i - a - b x_i) \\cdot (-1)} = -2 \\frac{1}{N} \\sum_{i=1}^N{(y_i - \\hat{y_i})}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial{MSE}}{\\partial{b}} = \\frac{\\partial{MSE}}{\\partial{\\hat{y_i}}} \\cdot \\frac{\\partial{\\hat{y_i}}}{\\partial{b}} = \\frac{1}{N} \\sum_{i=1}^N{2(y_i - a - b x_i) \\cdot (-x_i)} = -2 \\frac{1}{N} \\sum_{i=1}^N{x_i (y_i - \\hat{y_i})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6coSXbUz4h1F",
    "outputId": "5e621cf5-a6ca-4eab-ba49-d5727c2e558b"
   },
   "outputs": [],
   "source": [
    "# Computes gradients for both \"a\" and \"b\" parameters\n",
    "a_grad = -2 * error.mean()\n",
    "b_grad = -2 * (x_train * error).mean()\n",
    "print(a_grad, b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz6DZyBCs9sX"
   },
   "source": [
    "### Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UzaeN06rz5Ok"
   },
   "source": [
    "In the final step, we **use the gradients to update** the parameters. Since we are trying to **minimize** our **losses**, we **reverse the sign** of the gradient for the update.\n",
    "\n",
    "There is still another parameter to consider: the **learning rate**, denoted by the *Greek letter* **eta** (that looks like the letter **n**), which is the **multiplicative factor** that we need to apply to the gradient for the parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmBroXjJ0L4I"
   },
   "source": [
    "**Parameters**:\n",
    "\n",
    "$$\n",
    "\\large a = a - \\eta \\frac{\\partial{MSE}}{\\partial{a}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large b = b - \\eta \\frac{\\partial{MSE}}{\\partial{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU6sjDLx0YUq"
   },
   "source": [
    "How to choose a learning rate? That is a topic on its own and beyond the scope of this tutorial as well.\n",
    "\n",
    "Let's start with a value of **0.1** (which is a relatively *big value*, as far as learning rates are concerned!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "XSEigxDH5LJh",
    "outputId": "3be38315-3c59-431a-a64d-2ef83314d45f"
   },
   "outputs": [],
   "source": [
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "print(a, b)\n",
    "# Updates parameters using gradients and the learning rate\n",
    "a = a - lr * a_grad\n",
    "b = b - lr * b_grad\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIUmWPfKtAm1"
   },
   "source": [
    "### Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvwtAKpd0frZ"
   },
   "source": [
    "Now we use the **updated parameters** to go back to **Step 1** and restart the process.\n",
    "\n",
    "---\n",
    "\n",
    "*An **epoch is complete whenever every point has been already used for computing the loss**. For **batch** gradient descent, this is trivial, as it uses all points for computing the loss — **one epoch** is the same as **one update**. For **stochastic** gradient descent, **one epoch** means **N updates**, while for **mini-batch** (of size n), **one epoch** has **N/n updates**.*\n",
    "\n",
    "---\n",
    "\n",
    "Repeating this process over and over, for **many epochs**, is, in a nutshell, **training** a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: put the previous pieces of code together and loop over many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyLGYSSLtCeX"
   },
   "outputs": [],
   "source": [
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BwwqUJWv5X0s",
    "outputId": "6897f603-10b7-4eb7-8f97-597cacab4e33"
   },
   "outputs": [],
   "source": [
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtH7USnV76Mz"
   },
   "source": [
    "Just keep in mind that, if you **don’t** use batch gradient descent (our example does),you’ll have to write an **inner loop** to perform the **five training steps** for either each **individual point** (**stochastic**) or **n points** (**mini-batch**). We’ll see a mini-batch example later down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATtSr6pR8Oed"
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ke4Bw5wI8Q5R"
   },
   "source": [
    "Just to make sure we haven’t done any mistakes in our code, we can use *Scikit-Learn’s Linear Regression* to fit the model and compare the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XUsUvW808QLv",
    "outputId": "6f2000a1-e4d3-42c6-f1c3-f231b49bfe9e"
   },
   "outputs": [],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jh8Sc2ff8k7M"
   },
   "source": [
    "They **match** up to 6 decimal places — we have a *fully working implementation of linear regression* using Numpy.\n",
    "\n",
    "**Numpy?! Wait a minute… I thought this tutorial was about PyTorch!**\n",
    "\n",
    "Yes, it is, but this served **two purposes**: *first*, to introduce the **structure** of our task, which will remain largely the same and, *second*, to show you the main **pain points** so you can fully appreciate how much PyTorch makes your life easier :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-p8xcQKo9pDQ"
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bKKlGN9BaBa"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet torchviz\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2usggtDH9r2e"
   },
   "source": [
    "First, we need to cover a **few basic concepts** that may throw you off-balance if you don’t grasp them well enough before going full-force on modeling.\n",
    "\n",
    "In Deep Learning, we see **tensors** everywhere. Well, Google’s framework is called *TensorFlow* for a reason! *What is a tensor, anyway*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwyU03EC-zPj"
   },
   "source": [
    "### Tensors\n",
    "\n",
    "In *Numpy*, you may have an **array** that has **three dimensions**, right? That is, technically speaking, a **tensor**.\n",
    "\n",
    "A **scalar** (a single number) has **zero** dimensions, a **vector has one** dimension, a **matrix has two** dimensions and a **tensor has three or more dimensions**. That’s it!\n",
    "\n",
    "But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, **everything is either a scalar or a tensor**.\n",
    "\n",
    "![alt text](http://karlstratos.com/drawings/linear_dogs.jpg)\n",
    "Tensors are just higher-dimensional matrices :-) [Source](http://karlstratos.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPdIb6pIXrwW"
   },
   "source": [
    "You can create **tensors** in PyTorch pretty much the same way you create **arrays** in Numpy. Using [**tensor()**](https://pytorch.org/docs/stable/torch.html#torch.tensor) you can create either a scalar or a tensor.\n",
    "\n",
    "PyTorch's tensors have equivalent functions as its Numpy counterparts, like: [**ones()**](https://pytorch.org/docs/stable/torch.html#torch.ones), [**zeros()**](https://pytorch.org/docs/stable/torch.html#torch.zeros), [**rand()**](https://pytorch.org/docs/stable/torch.html#torch.rand), [**randn()**](https://pytorch.org/docs/stable/torch.html#torch.randn) and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "KeV7KOyBUi2S",
    "outputId": "6ad3ee7d-1f16-4285-d2be-60cf65f51267"
   },
   "outputs": [],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUp_UkLOcApC"
   },
   "source": [
    "You can get the shape of a tensor using its [**size()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size) method or its **shape** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pumy8ocYbpcC",
    "outputId": "8c0c3113-8653-49fb-91dd-2eedac65587c"
   },
   "outputs": [],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-9UB154cIsr"
   },
   "source": [
    "You can also reshape a tensor using its [**reshape()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape) or [**view()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) methods.\n",
    "\n",
    "Beware: these methods create a new tensor with the desired shape that **shares the underlying data** with the original tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dd5eSRi-XObM",
    "outputId": "6871812c-0fba-4462-92ec-8bc437ac0dea"
   },
   "outputs": [],
   "source": [
    "new_tensor1 = tensor.reshape(2, -1)\n",
    "new_tensor2 = tensor.view(2, -1)\n",
    "print(new_tensor1.shape, new_tensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCt37k99g2nm"
   },
   "source": [
    "If you want to copy all data for real, that is, duplicate it in memory, you should use either its [**copy_()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_) or [**clone()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q67_KSCSAAT1"
   },
   "source": [
    "### Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfRQIc19AEYA"
   },
   "source": [
    "”*How do we go from Numpy’s arrays to PyTorch’s tensors*”, you ask? \n",
    "\n",
    "That’s what [**from_numpy()**](https://pytorch.org/docs/stable/torch.html#torch.from_numpy) is good for. It returns a **CPU tensor**, though.\n",
    "\n",
    "You can also easily **cast** it to a lower precision (32-bit float) using [**float()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fgIjFB6gBUuz",
    "outputId": "53458278-0de2-466a-b12a-287c44fd089f"
   },
   "outputs": [],
   "source": [
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PmkqdBVRBTfo"
   },
   "source": [
    "“*But I want to use my fancy GPU…*”, you say.\n",
    "\n",
    "No worries, that’s what [**to()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) is good for. It sends your tensor to whatever **device** you specify, including your **GPU** (referred to as `cuda` or `cuda:0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Aceoy-DBy_F",
    "outputId": "7738d266-0475-4bb5-f0e7-8ad38673235d"
   },
   "outputs": [],
   "source": [
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to('cuda')\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to('cuda')\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hux68h4oCFt4"
   },
   "source": [
    "“*What if I want my code to fallback to CPU if no GPU is available?*”, you may be wondering… \n",
    "\n",
    "PyTorch got your back once more — you can use [**cuda.is_available()**](https://pytorch.org/docs/stable/cuda.html?highlight=is_available#torch.cuda.is_available) to find out if you have a GPU at your disposal and set your device accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q5qlPSxvCGKf",
    "outputId": "b6335d2c-fc9f-4468-bcb8-057ec12d4d6d"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lsYIvjQDE7O"
   },
   "source": [
    "If you compare the **types** of both variables, you’ll get what you’d expect: `numpy.ndarray` for the first one and `torch.Tensor` for the second one.\n",
    "\n",
    "But where does your nice tensor “live”? In your CPU or your GPU? You can’t say… but if you use PyTorch’s **type()**, it will reveal its **location** — `torch.cuda.FloatTensor` — a GPU tensor in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QbECiJmaC8nt",
    "outputId": "f0eddad4-ca02-4fb1-ca5d-d663669f2b60"
   },
   "outputs": [],
   "source": [
    "print(x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XF0MnXHYDRF4"
   },
   "source": [
    "We can also go the other way around, turning tensors back into Numpy arrays, using [**numpy()**](https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy). It should be easy as `x_train_tensor.numpy()` but…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "x1uCOtZvDdT0",
    "outputId": "eb115e85-2c4a-4f9a-db93-703455df01cb"
   },
   "outputs": [],
   "source": [
    "x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNKoo-n0DlUP"
   },
   "source": [
    "Unfortunately, Numpy **cannot** handle GPU tensors… you need to make them CPU tensors first using [**cpu()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ouRBUpRaDfUG",
    "outputId": "d65544f4-bf1a-46cd-f4a1-7de5064f36e3"
   },
   "outputs": [],
   "source": [
    "x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pYm2AqND-_T"
   },
   "source": [
    "### Creating Tensor for Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCT2YOYjEGJ_"
   },
   "source": [
    "What distinguishes a *tensor* used for *data* — like the ones we’ve just created — from a **tensor** used as a (*trainable*) **parameter/weight**?\n",
    "\n",
    "The latter tensors require the **computation of its gradients**, so we can **update** their values (the parameters’ values, that is). That’s what the **`requires_grad=True`** argument is good for. It tells PyTorch we want it to compute gradients for us.\n",
    "\n",
    "You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right? Let's see how it goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6bSQp0fxEE3S",
    "outputId": "4024b6a4-bddb-4618-edd4-15ab6d737bba"
   },
   "outputs": [],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IGZWtu8hO8J"
   },
   "source": [
    "The chunk of code above creates two nice tensors for our parameters, gradients and all. But they are **CPU** tensors.\n",
    "\n",
    "What if we want **GPU** tensors? Let's try sending them to a `cuda` device..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5mC5QLl8EirI",
    "outputId": "384539dd-a1a9-401b-90fd-570ddaba15fd"
   },
   "outputs": [],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhVM8hQ0htgn"
   },
   "source": [
    "In this second chunk of code, we tried the **naive** approach of sending them to our GPU. We succeeded in sending them to another device, but we **”lost”** the **gradients** somehow… **THIS ONLY HAPPENS IF DEVICE IS NOT CPU!**\n",
    "\n",
    "Maybe we should **first** send our tensors to the **device** and only **then** use `requires_grad_()` method to set its `requires_grad` attribute to `True` in place.\n",
    "\n",
    "---\n",
    "\n",
    "*In PyTorch, every method that **ends** with an **underscore (_)** makes changes **in-place**, meaning, they will **modify** the underlying variable.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a03zDmi_ho-E",
    "outputId": "480d6e12-f6ce-4a7f-e68c-1deb13bcafed"
   },
   "outputs": [],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSXD9Bf4imIu"
   },
   "source": [
    "This approach worked just fine, but it is much better to **assign** tensors to a **device** at the moment of their **creation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vb866lTxiTiA",
    "outputId": "d9f2183e-08cd-4c6a-96bc-2a33d8907193"
   },
   "outputs": [],
   "source": [
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGp27LxNi5ZL"
   },
   "source": [
    "Much easier, right?\n",
    "\n",
    "Now that we know how to create tensors that require gradients, let’s see how PyTorch handles them — that’s the role of the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68GypByGi8RO"
   },
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPV5It5YjAvL"
   },
   "source": [
    "Autograd is PyTorch’s *automatic differentiation package*. Thanks to it, we **don’t need to worry** about partial derivatives, chain rule or anything like it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLIcMHgVo4CU"
   },
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQuMOgJo6FU"
   },
   "source": [
    "So, how do we tell PyTorch to do its thing and **compute all gradients**? That’s what [**backward()**](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) is good for.\n",
    "\n",
    "Do you remember the **starting point** for **computing the gradients**? It was the **loss**, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable, like, `loss.backward()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: re-implement steps 1 to 3 using tensors and `backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9OilmlLh9_d"
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "yhat = ...\n",
    "\n",
    "# Step 2    \n",
    "# How wrong is our model? That's the error! \n",
    "error = ...\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = ...\n",
    "\n",
    "# Step 3    \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "youD0_1Do9_E"
   },
   "source": [
    "### grad / zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2JJ_osngW5H"
   },
   "source": [
    "\n",
    "What about the **actual values** of the **gradients**? We can inspect them by looking at the [**grad**](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad) **attribute** of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SM1hYBxlhMoZ",
    "outputId": "b011ab9c-3180-4902-d7f8-3e4f2e0ec7e9"
   },
   "outputs": [],
   "source": [
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJL6O06ChLio"
   },
   "source": [
    "If you check the method’s documentation, it clearly states that **gradients are accumulated**. \n",
    "\n",
    "You can check this out by running the two code cells above again.\n",
    "\n",
    "So, every time we use the **gradients** to **update** the parameters, we need to **zero the gradients afterwards**. And that’s what [**zero_()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.zero_) is good for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3wh62U94i7-o",
    "outputId": "b1cbc4b7-486d-4731-aee1-c6f6a9e03af8"
   },
   "outputs": [],
   "source": [
    "a.grad.zero_(), b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dy5vKAOi8J1"
   },
   "source": [
    "So, let’s **ditch** the **manual computation of gradients** and use both `backward()` and `zero_()` methods instead.\n",
    "\n",
    "And, we are still missing **Step 4**, that is, **updating the parameters**. Let's include it as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: update the parameters using `grad` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "2GAkYJYniX42",
    "outputId": "9581d9ba-43ab-4715-b4fa-21d2523ff236"
   },
   "outputs": [],
   "source": [
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "yhat = a + b * x_train_tensor\n",
    "\n",
    "# Step 2    \n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train_tensor - yhat)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3    \n",
    "# No more manual computation of gradients! \n",
    "loss.backward()\n",
    "# Computes gradients for both \"a\" and \"b\" parameters\n",
    "# a_grad = -2 * error.mean()\n",
    "# b_grad = -2 * (x_train_tensor * error).mean()\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "# Step 4\n",
    "# Updates parameters using gradients and the learning rate\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSKnCj7Ul5w4"
   },
   "source": [
    "What happened?! In this first attempt, if we use the same update structure as in our *Numpy* code, we’ll get the weird **error** above but we can get a *hint* of what’s going on by looking at the tensor itself — once again we **“lost”** the **gradient** while reassigning the update results to our parameters. Thus, the `grad` attribute turns out to be `None` and it raises the error…\n",
    "\n",
    "We can change it slightly, using a familiar **in-place Python assignment** as a second attempt. Try commenting the first update attempt, uncomment the other two lines of code and re-run it... did it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34P_CoUIm7T2"
   },
   "source": [
    "---\n",
    "\n",
    "It turns out to be a case of ***“too much of a good thing”***. The culprit is PyTorch’s ability to build a **dynamic computation graph** from every **Python operation** that involves any **gradient-computing tensor** or its **dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "Don't worry, we’ll go deeper into the inner workings of the dynamic computation graph in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-spqiSkpG3_"
   },
   "source": [
    "### no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIhiT37ApJy3"
   },
   "source": [
    "So, how do we tell PyTorch to “**back off**” and let us **update our parameters** without messing up with its **fancy dynamic computation graph**? That’s what `torch.no_grad()` is good for. It allows us to **perform regular Python operations on tensors, independent of PyTorch’s computation graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rxkh-X1ui9qJ",
    "outputId": "c6d824f3-7862-4c83-ed99-9c77cf09de39"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    # Computes our model's predicted output\n",
    "    yhat = a + b * x_train_tensor\n",
    "\n",
    "    # Step 2    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train_tensor - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3    \n",
    "    # No more manual computation of gradients! \n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4\n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cfde7qBr-dLd"
   },
   "source": [
    "Finally, we managed to successfully run our model and get the **resulting parameters**. Surely enough, they **match** the ones we got in our *Numpy*-only implementation.\n",
    "\n",
    "Let's take a look at the **loss** at the end of the training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qyrLZ7bm-fu3",
    "outputId": "97a2db3b-cfca-4b11-961f-bd576fd4f635"
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shzUgjPH_hqq"
   },
   "source": [
    "What if we wanted to have it as a *Numpy* array? I guess we could just use **numpy()** again, right? (and **cpu()** as well, since our *loss* is in the `cuda` device..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "c-fSotmZ_s-9",
    "outputId": "ffcd3e1c-1351-447c-a26c-f00cc7b35296"
   },
   "outputs": [],
   "source": [
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Uwg76k-_6CH"
   },
   "source": [
    "What happened here? Unlike our *data tensors*, the **loss tensor** is actually computing gradients - and in order to use **numpy**, we need to [**detach()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.detach_) that tensor from the computation graph first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qWBiGTaLAVfj",
    "outputId": "52c5af8d-589a-4bb7-f8bf-9ea58f052b71"
   },
   "outputs": [],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVvbZu_6AZZU"
   },
   "source": [
    "This seems like **a lot of work**, there must be an easier way! And there is one indeed: we can use [**item()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item), for **tensors with a single element** or [**tolist()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.tolist) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCKOS0RlAx-U",
    "outputId": "17b0f86d-f542-490c-d63e-11f7fccadf82"
   },
   "outputs": [],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mL4F2K3KpadZ"
   },
   "source": [
    "## Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EamDiupxpjSp"
   },
   "source": [
    "---\n",
    "\n",
    "“Unfortunately, no one can be told what the dynamic computation graph is. You have to see it for yourself.” \n",
    "\n",
    "Morpheus\n",
    "\n",
    "---\n",
    "\n",
    "Jokes aside, I want **you** to **see the graph for yourself** too!\n",
    "\n",
    "The [PyTorchViz](https://github.com/szagoruyko/pytorchviz) package and its `make_dot(variable)` method allows us to easily visualize a graph associated with a given Python variable.\n",
    "\n",
    "So, let’s stick with the **bare minimum**: two (gradient computing) **tensors** for our parameters, predictions, errors and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "va5ZPnwdpg0W"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERIK1XVWqemS"
   },
   "source": [
    "Now let's plot the **computation graph** for the **yhat** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "eft1ShqTp8_k",
    "outputId": "faf12edf-4917-4541-cb83-fdd177da3558"
   },
   "outputs": [],
   "source": [
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We7c0NLkq07K"
   },
   "source": [
    "Let’s take a closer look at its components:\n",
    "\n",
    "* **blue boxes**: these correspond to the **tensors** we use as **parameters**, the ones we’re asking PyTorch to **compute gradients** for;\n",
    "\n",
    "* **gray box**: a **Python operation** that involves a **gradient-computing tensor or its dependencies**;\n",
    "\n",
    "* **green box**: the same as the gray box, except it is the **starting point for the computation** of gradients (assuming the `**backward()**` method is called from the **variable used to visualize** the graph)— they are computed from the **bottom-up** in a graph.\n",
    "\n",
    "Now, take a closer look at the **green box**: there are **two arrows** pointing to it, since it is **adding up two variables**, `a` and `b*x`. Seems obvious, right?\n",
    "\n",
    "Then, look at the **gray box** of the same graph: it is performing a **multiplication**, namely, `b*x`. But there is only **one arrow** pointing to it! The arrow comes from the **blue box** that corresponds to our parameter `b`.\n",
    "\n",
    "Why don’t we have a box for our **data x**? The answer is: **we do not compute gradients for it**! So, even though there are *more* tensors involved in the operations performed by the computation graph, it **only** shows **gradient-computing tensors and its dependencies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGrlUBcMriPJ"
   },
   "source": [
    "Try using the `make_dot` method to plot the **computation graph** of other variables, like `error` or `loss`.\n",
    "\n",
    "The **only difference** between them and the first one is the number of **intermediate steps (gray boxes)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "Z3T9goqOqIt3",
    "outputId": "0870bd87-a4c0-4f39-c667-8dcb1794b910"
   },
   "outputs": [],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPZuuf-wt5Yk"
   },
   "source": [
    "What would happen to the computation graph if we set **`requires_grad`** to **`False`** for our parameter **`a`**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnsSXOMctokE"
   },
   "outputs": [],
   "source": [
    "a_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a_nograd + b * x_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "zzActDPeuLjb",
    "outputId": "0e897d6b-23cb-4b0a-fe16-b7951275de51"
   },
   "outputs": [],
   "source": [
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxojZtzpuQ6W"
   },
   "source": [
    "Unsurprisingly, the **blue box** corresponding to the **parameter a** is no more! \n",
    "\n",
    "Simple enough: **no gradients, no graph**.\n",
    "\n",
    "The **best thing** about the *dynamic computing graph* is the fact that you can make it **as complex as you want it**. You can even use *control flow statements* (e.g., if statements) to **control the flow of the gradients** (obviously!) :-)\n",
    "\n",
    "Let's build a nonsensical, yet complex, computation graph just to make a point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wddV-foIuOcO"
   },
   "outputs": [],
   "source": [
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "if loss > 0:\n",
    "    yhat2 = b * x_train_tensor\n",
    "    error2 = y_train_tensor - yhat2\n",
    "\n",
    "loss += error2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "J6kP2dA-u6w4",
    "outputId": "6d532c4c-334d-41ac-bd06-c14863c6c53f"
   },
   "outputs": [],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LB6C0xscvB3o"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwemv-t3vIbJ"
   },
   "source": [
    "So far, we’ve been **manually** updating the parameters using the computed gradients. That’s probably fine for *two parameters*… but what if we had a **whole lot of them**?! We use one of PyTorch’s **optimizers**, like [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) or [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).\n",
    "\n",
    "In the code below, we create a *Stochastic Gradient Descent* (SGD) optimizer to update our parameters **a** and **b**.\n",
    "\n",
    "---\n",
    "\n",
    "Don’t be fooled by the **optimizer’s** name: if we use **all training data** at once for the update — as we are actually doing in the code — the optimizer is performing a **batch** gradient descent, despite of its name.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gOkDgwawBUR"
   },
   "outputs": [],
   "source": [
    "# Our parameters\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw8Sku2ev4ML"
   },
   "source": [
    "### step / zero_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3ljYgxLv52K"
   },
   "source": [
    "An optimizer takes the **parameters** we want to update, the **learning rate** we want to use (and possibly many other hyper-parameters as well!) and **performs the updates** through its [**`step()`**](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step) method.\n",
    "\n",
    "Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s [**`zero_grad()`**](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad) method and that’s it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement step 4 using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YcNkxhfEvDlT",
    "outputId": "884b32e2-6b7d-4b5c-fb64-8c7a58859a16"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = a + b * x_train_tensor\n",
    "\n",
    "    # Step 2\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3\n",
    "    loss.backward()    \n",
    "    \n",
    "    # Step 4\n",
    "    ...\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHwVdA0HxCON"
   },
   "source": [
    "Cool! We’ve *optimized* the **optimization** process :-) What’s left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAq8ytYfxK7w"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7f_eHQOxN5b"
   },
   "source": [
    "We now tackle the **loss computation**. As expected, PyTorch got us covered once again. There are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to choose from, depending on the task at hand. Since ours is a regression, we are using the [Mean Square Error (MSE)](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss) loss.\n",
    "\n",
    "---\n",
    "\n",
    "Notice that `nn.MSELoss` actually **creates a loss function** for us — **it is NOT the loss function itself**. Moreover, you can specify a **reduction method** to be applied, that is, **how do you want to aggregate the results for individual points** — you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eC4EH3rKxL1J",
    "outputId": "242f1c17-6ceb-44c7-c282-ef8ae4d4c702"
   },
   "outputs": [],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pq5dPX8zBZk4",
    "outputId": "0b5851f2-1d7e-4651-c31e-89c1645c651d"
   },
   "outputs": [],
   "source": [
    "fake_labels = torch.tensor([1., 2., 3.])\n",
    "fake_preds = torch.tensor([1., 3., 5.])\n",
    "\n",
    "loss_fn(fake_labels, fake_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcS4vSiTyG7U"
   },
   "source": [
    "We then **use** the created loss function to compute the loss given our **predictions** and our **labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: implement step 2 using the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SxmHENASx3-A",
    "outputId": "d18dc319-2f57-4f75-e2a8-bd50ed280fbb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # Step 2\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3\n",
    "    loss.backward() \n",
    "\n",
    "    # Step 4\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey-gt8k4yUX6"
   },
   "source": [
    "At this point, there’s only one piece of code left to change: the **predictions**. It is then time to introduce PyTorch’s way of implementing a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Fh8LOI0yYT7"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A03THT7YybBn"
   },
   "source": [
    "In PyTorch, a **model** is represented by a regular **Python class** that inherits from the [**Module**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "* **\\__init__(self)**: **it defines the parts that make up the model** —in our case, two parameters, **a** and **b**.\n",
    "\n",
    "* **forward(self, x)**: it performs the **actual computation**, that is, it **outputs a prediction**, given the input **x**.\n",
    "\n",
    "---\n",
    "\n",
    "You are **not** limited to defining parameters, though… **models can contain other models (or layers) as its attributes** as well, so you can easily nest them. We’ll see an example of this shortly as well.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "You should **NOT call the `forward(x)`** method, though. You should **call the whole model itself**, as in **`model(x)`** to perform a forward pass and output predictions.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s build a proper (yet simple) model for our regression task. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNMj2ScHyoXU"
   },
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4CN2oGK0G7z"
   },
   "source": [
    "### Parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBc6HURx0F2i"
   },
   "source": [
    "In the **\\__init__** method, we define our **two parameters**, **a** and **b**, using the [**Parameter()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter) class, to tell PyTorch these **tensors should be considered parameters of the model they are an attribute of**.\n",
    "\n",
    "Why should we care about that? By doing so, we can use our model’s [**parameters()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters) method to retrieve **an iterator over all model’s parameters**, even those parameters of **nested models**, that we can use to feed our optimizer (instead of building a list of parameters ourselves!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Nwf_kZPhz0Qw",
    "outputId": "db096c9b-02c0-4832-d3cb-d03d3f96e14f"
   },
   "outputs": [],
   "source": [
    "dummy = ManualLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-5BiAQs08Tn"
   },
   "source": [
    "Moreover, we can get the **current values for all parameters** using our model’s [**state_dict()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "htdYIsSD0q2G",
    "outputId": "7d5372c2-20de-4dd1-fabe-330b6e349b81"
   },
   "outputs": [],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWl41iKdJdP"
   },
   "source": [
    "### state_dict\n",
    "\n",
    "The **state_dict()** of a given model is simply a Python dictionary that **maps each layer / parameter to its corresponding tensor**. But only **learnable** parameters are included, as its purpose is to keep track of parameters that are going to be updated by the **optimizer**.\n",
    "\n",
    "The **optimizer** itself also has a **state_dict()**, which contains its internal state, as well as the hyperparameters used.\n",
    "\n",
    "It turns out **state_dicts** can also be used for **checkpointing** a model, as we will see later down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "fPlsgkPUemlh",
    "outputId": "b0f29d45-bb84-4fd4-9c69-9a7e17669450"
   },
   "outputs": [],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJk570c41MlG"
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlTBJ4H21WSj"
   },
   "source": [
    "**IMPORTANT**: we need to **send our model to the same device where the data is**. If our data is made of GPU tensors, our model must “live” inside the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nYJYMvYE1N-Q",
    "outputId": "b8b7eec5-2f8a-46b7-bdee-9e0d3c24d343"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QG2VxJhf1v_a"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbHFyO-22Amu"
   },
   "source": [
    "In PyTorch, models have a [**train()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train) method which, somewhat disappointingly, **does NOT perform a training step**. Its only purpose is to **set the model to training mode**. \n",
    "\n",
    "Why is this important? Some models may use mechanisms like [**Dropout**](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout), for instance, which have **distinct behaviors in training and evaluation phases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: implement step 1 using the model (do not forget to set its mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sl-Sd_1f1k--",
    "outputId": "4ab7f0cf-e77f-48dd-ddd9-f0ef9a7b7aab"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "# Now the optimizers uses the parameters from the model\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = ...\n",
    "    \n",
    "    # Step 2\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    # Step 3\n",
    "    loss.backward()\n",
    "    # Step 4\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cD_yXamT2V7r"
   },
   "source": [
    "Now, the printed statements will look like this — final values for parameters **a** and **b** are still the same, so everything is ok :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGrzI0tw2pUX"
   },
   "source": [
    "### Nested Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hpp-gTuB23xK"
   },
   "source": [
    "In our model, we manually created two parameters to perform a linear regression. Let’s use PyTorch’s [**Linear**](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) layer as an attribute of our own, thus creating a nested model.\n",
    "\n",
    "Even though this clearly is a contrived example, as we are pretty much wrapping the underlying model without adding anything useful (or, at all!) to it, it illustrates well the concept.\n",
    "\n",
    "In the **\\__init__** method, we created an attribute that contains our **nested `Linear` model**.\n",
    "\n",
    "In the **forward()** method, we **call the nested model itself** to perform the forward pass (we should **not** call self.linear.forward(x)!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: set a Linear layer up for our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amK4WCg72rVB"
   },
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = ...\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTaDWLJq3djj"
   },
   "source": [
    "Now, if we call the **parameters()** method of this model, **PyTorch will figure the parameters of its attributes in a recursive way**.\n",
    "\n",
    "You can also add new `Linear` attributes and, even if you don’t use them at all in the forward pass, they will **still** be listed under `parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "396v3N5A3ONO",
    "outputId": "c5ce1dab-b439-477e-cbe5-5bd3b862596f"
   },
   "outputs": [],
   "source": [
    "dummy = LayerLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lRDodDDg3XEs",
    "outputId": "196133f9-22e0-43ac-b6f4-f2203c388255"
   },
   "outputs": [],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KcS2nS2LxpI"
   },
   "source": [
    "### Layers\n",
    "\n",
    "There are **MANY** different layers that can be uses in PyTorch:\n",
    "- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "- [Pooling Layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)\n",
    "- [Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
    "- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
    "- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
    "- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)\n",
    "- [Sparse Layers (embbedings)](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n",
    "- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)\n",
    "- [DataParallel Layers (multi-GPU)](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n",
    "- [Flatten Layer](https://pytorch.org/docs/stable/nn.html#flatten)\n",
    "\n",
    "We have just used a **Linear** layer. Later on this tutorial, we will use some other layers, like **Convolution**, **Pooling** and **Non-linear Activations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7G3-9GX2sRe"
   },
   "source": [
    "### Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G3evlAo37Gj"
   },
   "source": [
    "Our model was simple enough… You may be thinking: “*why even bother to build a class for it?!*” Well, you have a point…\n",
    "\n",
    "For **straightforward models**, that use **run-of-the-mill layers**, where the output of a layer is sequentially fed as an input to the next, we can use a, er… [**Sequential**](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) model :-)\n",
    "\n",
    "In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mt9ssm5w2vQa"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZt_4Z3U4KpE"
   },
   "source": [
    "Simple enough, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgJ1Oi1R4RAF"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ_wUvVA4TEP"
   },
   "source": [
    "So far, we’ve defined:\n",
    "* an **optimizer**\n",
    "\n",
    "* a **loss function**\n",
    "\n",
    "* a **model**\n",
    "\n",
    "Scroll up a bit and take a quick look at the code inside the loop. Would it **change** if we were using a **different optimizer**, or **loss**, or even **model**? If not, how can we make it more generic?\n",
    "\n",
    "Well, I guess we could say all these lines of code **perform a training step**, given those **three elements** (optimizer, loss and model),the **features** and the **labels**.\n",
    "\n",
    "So, how about **writing a function that takes those three elements** and **returns another function that performs a training step**, taking a set of features and labels as arguments and returning the corresponding loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH-nhgSY4SSC"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Step 1: Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Step 2: Computes loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3: Computes gradients\n",
    "        loss.backward()\n",
    "        # Step 4: Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1z-JIko4ysl"
   },
   "source": [
    "Then we can use this general-purpose function to build a **train_step()** function to be called inside our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4F60DNI64zBk",
    "outputId": "16339fc3-5210-4ad8-e5cd-151a83fdb656"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "# model = ManualLinearRegression().to(device)\n",
    "# model = LayerLinearRegression().to(device)\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkqKUEaj5EOi"
   },
   "source": [
    "Now our code should look like this… see how **tiny** the training loop is now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2ysa76u44nX",
    "outputId": "114af066-bcbd-4b2c-99ea-ef10165e3e3c"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "jOnHnxJQDU5C",
    "outputId": "0ba82b23-75f9-41bb-939b-7f432157aa94"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[:200])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWRFuU3R5zqs"
   },
   "source": [
    "Let’s give our training loop a rest and focus on our **data** for a while… so far, we’ve simply used our *Numpy arrays* turned **PyTorch tensors**. But we can do better, we can build a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VcHv7EZ56PR"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUdT1l_Q6Dx_"
   },
   "source": [
    "In PyTorch, a **dataset** is represented by a regular **Python class** that inherits from the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. You can think of it as a kind of a Python **list of tuples**, each tuple corresponding to **one point (features, label)**.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "* **\\__init__(self)**: it takes **whatever arguments** needed to build a **list of tuples** — it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n",
    "\n",
    "* **\\__get_item__(self, index)**: it allows the dataset to be **indexed**, so it can work like a list (`dataset[i]`) — it must **return a tuple (features, label)** corresponding to the requested data point. We can either return the **corresponding slices** of our **pre-loaded** dataset or tensors or, as mentioned above, **load them on demand** (like in this [example](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)).\n",
    "\n",
    "* **\\__len__(self)**: it should simply return the **size** of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n",
    "\n",
    "---\n",
    "\n",
    "There is **no need to load the whole dataset in the constructor method** (\\__init__). If your **dataset is big** (tens of thousands of image files, for instance), loading it at once would not be memory efficient. It is recommended to **load them on demand** (whenever __get_item__ is called).\n",
    "\n",
    "---\n",
    "\n",
    "Let’s build a simple custom dataset that takes two tensors as arguments: one for the features, one for the labels. For any given index, our dataset class will return the corresponding slice of each of those tensors. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKu-E5xQ57pE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T2Lpj43e7Gpt",
    "outputId": "eebbd103-af3e-461c-8f71-365230c32156"
   },
   "outputs": [],
   "source": [
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zW1bTD4l7VAE"
   },
   "source": [
    "---\n",
    "\n",
    "Did you notice we built our **training tensors** out of Numpy arrays but we **did not send them to a device**? So, they are **CPU** tensors now! **Why**?\n",
    "\n",
    "We **don’t want our whole training data to be loaded into GPU tensors**, as we have been doing in our example so far, because **it takes up space** in our precious **graphics card’s RAM**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUwVUhKP75zt"
   },
   "source": [
    "### TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0-lPsQf770A"
   },
   "source": [
    "Besides, you may be thinking “*why go through all this trouble to wrap a couple of tensors in a class?*”. And, once again, you do have a point… if a dataset is nothing else but a **couple of tensors**, we can use PyTorch’s [**TensorDataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class, which will do pretty much what we did in our custom dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wIjLRR_666H2",
    "outputId": "d013459f-02cb-4eae-dcd6-19161da33a38"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bZZI1U37ylE"
   },
   "source": [
    "OK, fine, but then again, **why** are we building a dataset anyway? We’re doing it because we want to use a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLvd96o77-Rj"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JH_FSh4G8CM-"
   },
   "source": [
    "Until now, we have used the **whole training data** at every training step. It has been **batch gradient descent** all along. This is fine for our *ridiculously small dataset*, sure, but if we want to go serious about all this, we **must use mini-batch** gradient descent. Thus, we need mini-batches. Thus, we need to **slice** our dataset accordingly. \n",
    "\n",
    "Do you want to do it *manually*?! Me neither!\n",
    "\n",
    "So we use PyTorch’s [**DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class for this job. We tell it which **dataset** to use (the one we just built in the previous section), the desired **mini-batch size** and if we’d like to **shuffle** it or not. That’s it!\n",
    "\n",
    "Our **loader** will behave like an **iterator**, so we can **loop over it** and **fetch a different mini-batch** every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ana8r6AN7_hv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHHtcorn8dMq"
   },
   "source": [
    "To retrieve a sample mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "sPX5ggEC8bXU",
    "outputId": "70fed320-7846-42b7-94f6-2264fb8ee222"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrhy1id98iVj"
   },
   "source": [
    "How does this change our training loop? Let’s check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "madfwBfC8fjs",
    "outputId": "e1dc404e-08be-456e-ca1c-4da251095ce7"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "0_QL4Q2aEGR7",
    "outputId": "a557dba9-428e-416e-90a7-397655af0549"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs (?)')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mc06yWq85XN"
   },
   "source": [
    "Did you notice it is taking **longer** to train now? Can you guess **why**?\n",
    "\n",
    "Two things are different now: not only we have an **inner loop** to load each and every **mini-batch** from our **DataLoader** but, more importantly, we are now **sending only one mini-batch to the device**.\n",
    "\n",
    "---\n",
    "\n",
    "For bigger datasets, **loading data sample by sample** (into a **CPU** tensor) using **Dataset’s \\__get_item__** and then **sending all samples** that belong to the **same mini-batch at once to your GPU** (device) is the way to go in order to make the **best use of your graphics card’s RAM**.\n",
    "\n",
    "Moreover, if you have **many GPUs** to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.\n",
    "\n",
    "---\n",
    "\n",
    "So far, we’ve focused on the **training data** only. We built a *dataset* and a *data loader* for it. We could do the same for the **validation** data, using the **split** we performed at the beginning of this post… or we could use **random_split** instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuUK7LBS9u8L"
   },
   "source": [
    "### random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6qndQid95al"
   },
   "source": [
    "PyTorch’s [**random_split()**](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) method is an easy and familiar way of performing a **training-validation split**. Just keep in mind that, in our example, we need to apply it to the **whole dataset** (not the *training* dataset we built in couple of sections ago).\n",
    "\n",
    "Then, for each subset of data, we build a corresponding DataLoader, so our code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_g3S4hx83pW"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pD8ItwP-TMC"
   },
   "source": [
    "Now we have a **data loader** for our **validation** set, so, it makes sense to use it for the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6iCpQ1B-YfS"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQCWnko4-az9"
   },
   "source": [
    "Now, we need to change the training loop to include the **evaluation of our model**, that is, computing the **validation loss**. The first step is to include another inner loop to handle the *mini-batches* that come from the *validation loader* , sending them to the same *device* as our model. Next, we make **predictions** using our model and compute the corresponding **loss**.\n",
    "\n",
    "That’s pretty much it, but there are **two small, yet important**, things to consider:\n",
    "\n",
    "* [**torch.no_grad()**](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad): even though it won’t make a difference in our simple model, it is a **good practice to wrap the validation inner loop with this context manager to disable any gradient calculation** that you may inadvertently trigger — **gradients belong in training**, not in validation steps;\n",
    "    \n",
    "* [**eval()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval): the only thing it does is **setting the model to evaluation mode** (just like its `train()` counterpart did), so the model can adjust its behavior regarding some operations, like [**Dropout**](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: use the validation loader to compute the average loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4bgvUJLI-Z_D",
    "outputId": "e574b60f-68e8-4465-f44d-6a3802e007d7"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Looping through epochs...\n",
    "for epoch in range(n_epochs):\n",
    "    # TRAINING\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "\n",
    "    losses.append(np.mean(batch_losses))\n",
    "        \n",
    "    # VALIDATION\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_batch_losses = []\n",
    "\n",
    "        ...\n",
    "        \n",
    "        val_losses.append(np.mean(val_batch_losses))\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YW8xqQSifxv9"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbeACyd5mYbP"
   },
   "source": [
    "The training loop should be a stable structure, so we can organize it into functions as well...\n",
    "Let's build a function for **validation** and another one for the **training loop** itself, training step and all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2ITAYVZf69n"
   },
   "outputs": [],
   "source": [
    "def validation(model, loss_fn, val_loader):\n",
    "    # Figures device from where the model parameters (hence, the model) are\n",
    "    device = next(model.parameters()).device.type\n",
    "    val_losses = []\n",
    "\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_batch_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # sets model to EVAL mode\n",
    "            model.eval()\n",
    "\n",
    "            # make predictions\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "            val_batch_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(val_batch_losses))\n",
    "\n",
    "    return val_losses\n",
    "\n",
    "\n",
    "def train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader=None):\n",
    "    # Figures device from where the model parameters (hence, the model) are\n",
    "    device = next(model.parameters()).device.type\n",
    "    # Creates the train_step function for our model, loss function and optimizer\n",
    "    train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # TRAINING\n",
    "        batch_losses = []\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "        losses.append(np.mean(batch_losses))\n",
    "\n",
    "        # VALIDATION\n",
    "        if val_loader is not None:\n",
    "            val_loss = validation(model, loss_fn, val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print(\"Epoch {} complete...\".format(epoch))\n",
    "\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ztnViVdOiNUS",
    "outputId": "f0676564-7a80-4e17-e001-ce55b29ccd10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses, val_losses = train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "YiAyyGoPEp1l",
    "outputId": "2d823dca-e37a-4e21-e752-48b543f2b6e2"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARYN8POSHxPa"
   },
   "source": [
    "\"*Wait, there is something weird with this plot...*\", you say. You're right, the **validation loss** is **smaller** than the **training loss**. Shouldn't it be the other way around?! Well, generally speaking, *YES*, it should... but you can learn more about situations where this *swap* happens at this great [post](pyimg.co/kku35)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwmMttjrfl3j"
   },
   "source": [
    "## TorchVision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oQof5AafziT"
   },
   "source": [
    "[Torchvision](https://pytorch.org/docs/stable/torchvision/index.html) is a package containing popular datasets, model architectures and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kodc3GopgsEL"
   },
   "source": [
    "### Datasets\n",
    "\n",
    "All [datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) included in torchvision are subclasses of the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class we've seen earlier in this tutorial, so we can stick with the [**DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) as well.\n",
    "\n",
    "We'll work with the canonical dataset for computer vision tutorials: [**MNIST**](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist).\n",
    "\n",
    "**Training Set**\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "**Test Set**\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JbTt-k5Jhm7a",
    "outputId": "cf511d6b-7926-44c6-a87d-5e294a012d5f"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "mnist_train = datasets.MNIST('./mnist', train=True, download=True)\n",
    "mnist_test = datasets.MNIST('./mnist', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "6ocJ8tdbhxTV",
    "outputId": "eede568c-1bd5-45b6-f4d2-283f765f7d4b"
   },
   "outputs": [],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vKSUAcReh_fX",
    "outputId": "c16b1ceb-6e0d-4fda-ef6a-a9d3966ea126"
   },
   "outputs": [],
   "source": [
    "mnist_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LB39bOA3iEK7",
    "outputId": "2df7ff7d-667b-423b-a7ab-be0fbf10a8cd"
   },
   "outputs": [],
   "source": [
    "mnist_train.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXxOwHui-SDy"
   },
   "source": [
    "There are 60k 28x28 images and the targets are the corresponding digits.\n",
    "\n",
    "Let's take a look at the first sample, which is a **5**. You'll notice that, somewhat unsurprisingly, the sample is a **tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "SSzft4CGiesa",
    "outputId": "9f50120b-c23f-4ac7-c717-27df098ce1ee"
   },
   "outputs": [],
   "source": [
    "sample_tensor = mnist_train.data[0]\n",
    "plt.imshow(sample_tensor)\n",
    "print(sample_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5a_B_srP_RLL"
   },
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxQe08L7_TJ3"
   },
   "source": [
    "Torchvision has some common image transformations on its [**transforms**](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms) module. It is important to realize there are two main groups of transformations:\n",
    "\n",
    "- transformations based on [**PIL images**](https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-pil-image)\n",
    "- transformations based on [**Tensors**](https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-torch-tensor)\n",
    "\n",
    "Obviously, there are transformations to convert from tensors [**ToPILImage**](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToPILImage) and from PIL image [**ToTensor**](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor).\n",
    "\n",
    "Let's try converting our sample tensor into a sample (PIL) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "P4cEG1xiioLj",
    "outputId": "ee9c60cb-db88-476a-8327-5d87f506006e"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "sample_img = ToPILImage()(sample_tensor)\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5QUm376BOMT"
   },
   "source": [
    "### Transforms on PIL Image\n",
    "\n",
    "These transforms include the typical things you'd like to do with an image: [Resize](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize), [CenterCrop](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.CenterCrop), [GrayScale](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Grayscale), [RandomHorizontalFlip](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip), to name a few.\n",
    "\n",
    "They take a **PIL Image** as inputs, not tensors. So, let's use our sample image from the previous step and try some **random horizontal flipping**. But, just to make sure we flip, let's ditch the randomness and make it flip a 100% of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfdqWGGvjgHs"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "\n",
    "flipper = RandomHorizontalFlip(p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "4L638lufj-cW",
    "outputId": "103336af-33b6-45a1-d419-949ca5ea36a1"
   },
   "outputs": [],
   "source": [
    "flipped_img = flipper(sample_img)\n",
    "plt.imshow(flipped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "muk16q5ECi0V"
   },
   "source": [
    "Ok, we have a **flipped 5** now.\n",
    "\n",
    "Let's take a look at the other group of transformations now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKJ6weWFCsXT"
   },
   "source": [
    "### Transforms on Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtYjawYQCvGy"
   },
   "source": [
    "These are only three transforms that take tensors as inputs: [LinearTransformation](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.LinearTransformation), [Normalize](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize) and [RandomErasing](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomErasing) (although I believe this one was a better fit for the other group of transforms...).\n",
    "\n",
    "Let's apply a **Normalize** transform to our **flipped 5**. But, in order to be able to do that, we need to make it back into a tensor using **ToTensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UQ809kn-kDs2",
    "outputId": "b4033769-d44a-4128-c951-f8b177579a91"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "tensorizer = ToTensor()\n",
    "img_tensor = tensorizer(flipped_img)\n",
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O5JhbrDTDq3y",
    "outputId": "aa478059-cdf2-42d6-f5c3-86df80bb3c2b"
   },
   "outputs": [],
   "source": [
    "img_tensor.min(), img_tensor.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWYsWzG-F46B"
   },
   "source": [
    "### Normalize Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvagqAgLD2I8"
   },
   "source": [
    "From our image tensor, we see its values are in the **[0, 1]** range.\n",
    "\n",
    "Usually, when dealing with neural networks, it is better to have our inputs in a symmetrical range, like **[-1, 1]**.\n",
    "\n",
    "And that's why we are going to use the **Normalize** transform. From PyTorch's extensive documentation, we get:\n",
    "\n",
    "`Normalize a tensor image with mean and standard deviation.`\n",
    "\n",
    "`Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e.`\n",
    "\n",
    "`input[channel] = (input[channel] - mean[channel]) / std[channel]`\n",
    "\n",
    "So, if we would like to map our [0, 1] range into [-1, 1], we can set our **`mean` to 0.5** and our **`std` to 0.5** as well.\n",
    "\n",
    "This way, we get:\n",
    "- 0 input is transformed into (0 - .5)/.5 = -1\n",
    "- 1 input is transformed into (1 - .5)/.5 = 1\n",
    "\n",
    "---\n",
    "\n",
    "Even though the transform is called **Normalize**, what we have just done with the inputs is actually a **min-max scaling**. We have **NOT** computed the **true mean** and **true standard deviation**, we have just made them equals 0.5 for our convenience.\n",
    "\n",
    "Had we use the true values for both mean and standard deviation, we would have achieved a **standardization**, that is, our data would have **zero mean** and **unit standard deviation**.\n",
    "\n",
    "---\n",
    "\n",
    "Our images have only **1 channel**, so we only need to specify one element in each tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1a3qc1VYlCPu",
    "outputId": "95de66cb-cc2c-487d-b2c0-f286648667d5"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "normalizer = Normalize(mean=(.5,), std=(.5,))\n",
    "normalized_tensor = normalizer(img_tensor)\n",
    "normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kMKtP9VPF8zM",
    "outputId": "33903614-88f3-4e37-e1e3-07240d22723e"
   },
   "outputs": [],
   "source": [
    "normalized_tensor.min(), normalized_tensor.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTUzjwioGD6O"
   },
   "source": [
    "The range is [-1, 1] now.\n",
    "\n",
    "**Mission accomplished!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoXSyugtGJHx"
   },
   "source": [
    "### Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP1_2p6tGOW8"
   },
   "source": [
    "Sure enough, we don't need to do transformations one by one: we can use [**Compose**](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose) for that.\n",
    "\n",
    "This is straightforward: line all desired transformations up in a list. This is pretty much the same as a Pipeline in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaIEJN3TldtN"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "composer = Compose([ToPILImage(),\n",
    "                    RandomHorizontalFlip(p=1.0),\n",
    "                    ToTensor(),\n",
    "                    Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "composed_tensor = composer(sample_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLTM0uiPGyXJ"
   },
   "source": [
    "The resulting tensor should be exactly the same as the tensor we have transformed step-by-step... let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Falqqiu_mIQu",
    "outputId": "e6285ab4-827d-40d6-93dd-8a7fa7d4d84d"
   },
   "outputs": [],
   "source": [
    "(composed_tensor == normalized_tensor).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOgaUUeRHf5P"
   },
   "source": [
    "### Transforming Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-Yk2bRDHiJD"
   },
   "source": [
    "The best thing about composing transforms is the fact you can apply them whenever a given data point is being sampled from the dataset.\n",
    "\n",
    "Remember the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class, which torchvision datasets use as parent class? It is possible to call a composed transformation, just like we did, inside the **\\__get_item__(self, index)** method.\n",
    "\n",
    "And that's exactly what torchvision datasets do! You can specify a **transform** argument and, whenever your [**DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) fetches some samples, they will be transformed already! **Beautiful**, uh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NTbs6vToM5S"
   },
   "outputs": [],
   "source": [
    "new_composer = Compose([RandomHorizontalFlip(p=.5),\n",
    "                        ToTensor(),\n",
    "                        Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "mnist_train = datasets.MNIST('./mnist', train=True, download=True, transform=new_composer)\n",
    "mnist_test = datasets.MNIST('./mnist', train=False, download=True, transform=new_composer)\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(mnist_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sz3fpPMar7gp",
    "outputId": "6f4ebbfd-659c-4442-c6c1-938b8d7f351d"
   },
   "outputs": [],
   "source": [
    "transformed_sample = next(iter(train_loader))[0][0]\n",
    "transformed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HscbwyVCI7XR",
    "outputId": "de7758d6-e020-4864-f538-46b84caeb244"
   },
   "outputs": [],
   "source": [
    "transformed_sample.min(), transformed_sample.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KLWNERUJHd3"
   },
   "source": [
    "Just as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XILRTwMhMgDo"
   },
   "source": [
    "## A Simple Neural Network\n",
    "\n",
    "We will use the **MNIST** dataset and build a simple neural network to try classify the hand-written digits.\n",
    "\n",
    "Besides, let's keep it simple and build a **Sequential** model, just like we did with our linear regression.\n",
    "\n",
    "Our input images have 28x28 pixels, that is, 784 pixels in total. Our targets are 10 different classes (digits 0 to 9). So, our model can be structured as follows:\n",
    "\n",
    "- **input layer**: 784 units\n",
    "- **hidden layer(s) and non-linear activations**: we can get creative :-)\n",
    "- **output layer**: 10 units\n",
    "\n",
    "For now, let's use **one hidden layer** with **50 units** and let's use a [**ReLU**](https://pytorch.org/docs/stable/nn.html#relu) as non-linear activation.\n",
    "\n",
    "Since our inputs are 2-dimensional (28x28) and our model has 784 inputs, we still need to add a [**Flatten**](https://pytorch.org/docs/stable/nn.html#flatten) layer at the beginning.\n",
    "\n",
    "So, our model would look like this - let's use [**add_module**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.add_module) to **name** our layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "n8J4I8V-PaQC",
    "outputId": "85327f96-5bd4-4048-d13d-6d358b75908f"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "model.add_module('linear1', nn.Linear(784, 50))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('output', nn.Linear(50, 10))\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLVGcbLP9Lhj"
   },
   "source": [
    "If we use model's [**named_modules()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.named_modules) method, we can retrieve a list of all modules and the model itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "eFYGJbnO9Rsi",
    "outputId": "90f4bac2-b725-4c4c-9fa2-fbf1c7ad3ce8"
   },
   "outputs": [],
   "source": [
    "list(model.named_modules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vu2uEYbA7xxc"
   },
   "source": [
    "And we can use the layers / modules names using dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-MM-6xkq6Tmu",
    "outputId": "083e227f-9c57-4e82-f7f5-c659973067ad"
   },
   "outputs": [],
   "source": [
    "model.linear1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adqDNfrb8ZUE"
   },
   "source": [
    "We can also get its corresponding **weights**: `model.linear1.weight`\n",
    "\n",
    "Let's take a look at the weights, plotting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "aWrba9dJ71MK",
    "outputId": "b838ef2b-d036-4a1c-c1d3-1ac90b29b07c"
   },
   "outputs": [],
   "source": [
    "weights = model.linear1.weight\n",
    "plt.hist(weights.detach().cpu().numpy().reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "grGaAust-Fst",
    "outputId": "edd85264-1d10-4710-8500-768f18117784"
   },
   "outputs": [],
   "source": [
    "print(weights.min(), weights.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uNp_47r91xX"
   },
   "source": [
    "The linear layer has **39200 weights** and we can see they are **uniformly distributed in [-.0357, .0357] range**. Do you think this is a **good initialization scheme**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8Tgd-bh-g9n"
   },
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NduRvhTo-kZM"
   },
   "source": [
    "Initialization schemes are a big deal! It goes beyond the scope of this tutorial to go into more details, but we'll see how we can **initialize the weights** of a given layer in our neural network.\n",
    "\n",
    "For more details on **weight initialization schemes**, you can check my [post](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404) out :-)\n",
    "\n",
    "PyTorch has an [**init**](https://pytorch.org/docs/stable/nn.init.html#torch-nn-init) module that has the most common initialization schemes, such as [**kaiming_uniform**](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) and [**kaiming_normal**](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_) - also known as **He initialization** - the recommended scheme for using with **ReLU** activation function.\n",
    "\n",
    "Let's use **kaiming_normal_** (notice the **_** - we are making changes **in place**!) to initialize the weights of our linear layer. And let's zero our biases too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "J5BXrOxF5z8j",
    "outputId": "6439b067-e29a-4447-dbe5-09d9f1a34df7"
   },
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(model.linear1.weight, mode='fan_in', nonlinearity='relu')\n",
    "nn.init.constant_(model.linear1.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuRsawj1AZDR"
   },
   "source": [
    "Did it work? Let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "1jmWb3M-9Avr",
    "outputId": "f24c4727-e2b6-42e2-9a57-4cec1a9efae6"
   },
   "outputs": [],
   "source": [
    "weights = model.linear1.weight\n",
    "plt.hist(weights.detach().cpu().numpy().reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3jX9LJdAdBf"
   },
   "source": [
    "Awesome! We have succesfully initialized the weights of our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKxKYVspAkmG"
   },
   "source": [
    "### Softmax Layer for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-CXmxYUSPI7"
   },
   "source": [
    "You may be thinking: \"*this is a **multi-class classification** problem... where is the **softmax** layer at the end?*\"\n",
    "\n",
    "Well, you do have a point... but PyTorch makes things a bit confusing here... \n",
    "\n",
    "You **MAY** add a softmax layer at the end or, better yet, a [**LogSoftmax**](https://pytorch.org/docs/stable/nn.html#logsoftmax). If you **DO add it**, you **MUST** use the corresponding negative log-likelihood loss ([**NLLLoss**](https://pytorch.org/docs/stable/nn.html#nllloss)).\n",
    "\n",
    "**BUT**\n",
    "\n",
    "If you **DO NOT** add a **LogSoftmax** layer at the end, you **MUST** use the [**CrossEntropyLoss**](https://pytorch.org/docs/stable/nn.html#crossentropyloss) which, by itself, combines **LogSoftmax AND NLLLoss**.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, you have two options:\n",
    "- **do** add **LogSoftmax** layer at the end and use **NLLLoss**, or\n",
    "- just use **CrossEntropyLoss**\n",
    "\n",
    "---\n",
    "\n",
    "We use the latter, as it is simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWSdYngaTiob"
   },
   "source": [
    "### Training Step\n",
    "\n",
    "Remember the **training step** function we built earlier? Its arguments were:\n",
    "- a **model**: we have one, our neural network\n",
    "- a **loss function**: our problem is a classification and we have not added a softmax layer, so we must use **CrossEntropyLoss**\n",
    "- an **optimizer**: let's stick with **SGD**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IxkRwcEX6jL"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AB3Q5Xubxpb"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoI3vrWxb2Yb"
   },
   "source": [
    "We can also reuse the code from the **Evaluation** section to perform the model training - using 10 instead of 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "B4or2DZ9ZJwL",
    "outputId": "7900099d-f82c-4278-c914-1551f5f25168"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "losses, val_losses = train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "ZORdHKfBay9q",
    "outputId": "cfa80b4b-d136-445d-e68e-bd969bd414d2"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vm_spjrfb6YJ"
   },
   "source": [
    "---\n",
    "\n",
    "Isn't it **great**? We loaded the **MNIST dataset**, built a **neural network** model, defined the corresponding **loss function** and **reused everything else**.\n",
    "\n",
    "---\n",
    "\n",
    "What's left to do? We need to check our model's **accuracy** on the validation dataset. So, we make small changes to the **validation loop**, checking which class got the highest probability and matching it against our targets.\n",
    "\n",
    "We can use [**torch.max()**](https://pytorch.org/docs/stable/torch.html#torch.max) with `dim=1` to get both the maximum value (highest predicted probability) and its index (which corresponds to the predicted digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-p8fmk5LaT_Q",
    "outputId": "c3c6b83e-2376-46c3-86d0-ee6021d3c548"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_val, y_val in val_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        yhat = model(x_val)\n",
    "        \n",
    "        # this is PyTorch's version of argmax, but it returns a tuple: (max value, index of max value)\n",
    "        _, predicted = torch.max(yhat, 1)\n",
    "        # we get the size of the batch and add up to the total number of samples\n",
    "        total += y_val.size(0)\n",
    "        # we add how many samples got classified correctly\n",
    "        correct += (predicted == y_val).sum().item()\n",
    "        \n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3q9OG1Md7TV"
   },
   "source": [
    "**We got almost 95% accuracy**!\n",
    "\n",
    "But, can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dbs0SosifnFx"
   },
   "source": [
    "## A Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYbh6xG0frH3"
   },
   "source": [
    "It is time to try something a bit more sophisticated!\n",
    "\n",
    "Let's create our own model, inheriting from the [**Module**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. It will take one argument, the **number of features** we will create output in our convolution layer.\n",
    "\n",
    "We will use [**Conv2d**](https://pytorch.org/docs/stable/nn.html#conv2d) and [**MaxPool2d**](https://pytorch.org/docs/stable/nn.html#maxpool2d) layers, apart from the already familiar **Linear**, **ReLU** and **Flatten**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qq4e5S27fsvc"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        # Creates the convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature, out_channels=n_feature, kernel_size=5)\n",
    "        # Creates the linear layers\n",
    "        self.fc1 = nn.Linear(n_feature * 4 * 4, 50) # where do this 4 * 4 come from?! check it below\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        # Input dimension: (1, 28, 28)\n",
    "        # Output dimension: (n_feature, 24, 24) - we loose (5-1) pixels in each dimension due to the kernel and no padding\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Input dimension (n_feature, 24, 24)\n",
    "        # Output dimension (n_feature, 12, 12)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Input dimension (n_feature, 12, 12)\n",
    "        # Output dimension (n_feature, 8, 8)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Input dimension (n_feature, 8, 8)\n",
    "        # Output dimension (n_feature, 4, 4) - we loose (5-1) pixels in each dimension due to the kernel and no padding\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Input dimension (n_feature, 4, 4)\n",
    "        # Output dimension (n_feature * 4 * 4)\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "        # Input dimension (n_feature * 4 * 4)\n",
    "        # Output dimension (50)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Input dimension (50)\n",
    "        # Output dimension (10)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-OwXP4Rpb_K"
   },
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxFe_H7SqQU9"
   },
   "source": [
    "Maybe you noticed already, but our CNN model used a **mix** of **layers** from both **torch.nn** and **torch.nn.functional**.\n",
    "\n",
    "\"*What is the difference?*\", you ask?\n",
    "\n",
    "PyTorch's [**functional**](https://pytorch.org/docs/stable/nn.functional.html#torch-nn-functional) layers are, er... **functions**.\n",
    "\n",
    "Let's consider two different layers: **2d convolution** and **2d max pooling**. Both can be used as **layer** or **function**:\n",
    "- [**Conv2d**](https://pytorch.org/docs/stable/nn.html#conv2d) or [**F.conv2d**](https://pytorch.org/docs/stable/nn.functional.html#conv2d)\n",
    "- [**MaxPool2d**](https://pytorch.org/docs/stable/nn.html#maxpool2d) or [**F.max_pool2d**](https://pytorch.org/docs/stable/nn.functional.html#max-pool2d)\n",
    "\n",
    "Why did we use a **Conv2d layer** but a **F.max_pool2d function**?\n",
    "\n",
    "Well, on one hand, it is more **convenient** to use `F.max_pool2d(x, kernel_size=2)` instead of `nn.MaxPool2d(kernel_size=2)(x)`. It just looks better, I think...\n",
    "\n",
    "But the thing is, **max pooling layers** do not have **parameters/weights** to be learned! **Convolution layers, do!**\n",
    "\n",
    "Remember the **Nested Model** section? Every **model** or **layer** gets its **parameters** recursively accessed by the parent **model**, **provided they are attributes in the `__init__` method**!\n",
    "\n",
    "---\n",
    "\n",
    "So, it is pretty simple: does the layer has **parameters** that need to be learned?\n",
    "- **YES** - Use **layers** from **torch.nn** and make them **attributes of the parent model**\n",
    "- **NO** - feel free to use **functions** from **torch.nn.functional**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srS9dwSht5ms"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTsQCdRFt--e"
   },
   "source": [
    "### Exercise 9: set the components up to make a train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJD5f1SehHPN"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "qxLqnDjtoh_L",
    "outputId": "22fd4284-8d13-4a3c-f0c5-995a24e89024"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8HNf8JcuG7H"
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "You know the drill :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "qHH8U6SvhKyp",
    "outputId": "442c2ef9-5414-47d6-fa1d-83838478b772"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "losses, val_losses = train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "SsDKVvuLhPUT",
    "outputId": "c74d08cd-0467-41c8-bb61-52ae4cd9c8e9"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6ZUzCw2ShPYK",
    "outputId": "aaa143e4-630a-4f2e-e272-9fe253e0549d"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_val, y_val in val_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        yhat = model(x_val)\n",
    "        \n",
    "        # this is PyTorch's version of argmax, but it returns a tuple: (max value, index of max value)\n",
    "        _, predicted = torch.max(yhat, 1)\n",
    "        # we get the size of the batch and add up to the total number of samples\n",
    "        total += y_val.size(0)\n",
    "        # we add how many samples got classified correctly\n",
    "        correct += (predicted == y_val).sum().item()\n",
    "        \n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9dwDYZipoxB"
   },
   "source": [
    "**Awesome! We are close to 98% accuracy now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LB65P7O2Mgx"
   },
   "source": [
    "### Visualizing Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LjRN5t5nVTd"
   },
   "source": [
    "One of the cool things about Convolution Neural Networks is that its **features** or **filters** can be visualized, so we can have a glimpse of **what the network is looking for** inside the input images :-)\n",
    "\n",
    "We can fetch the **weights** of a convolution layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "iWWZqm5_orhX",
    "outputId": "7ddfb539-f1c4-4c76-819a-ef5f564dc738"
   },
   "outputs": [],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2cnqfORoy6E"
   },
   "source": [
    "This tensor is 6 x 5 x 5... why is that? It has **6 features or filters** (we configured it that way) and each one is **5 x 5** because that's the **kernel size** we defined. No surprises here!\n",
    "\n",
    "But we can think of each one of this 5 x 5 tensors as **an image**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "W8R9qvqX2RzK",
    "outputId": "8b0dd0b6-e709-4f19-ca48-48ae192ec30e"
   },
   "outputs": [],
   "source": [
    "# Code adapted from \"How to Visualize Filters and Feature Maps in Convolutional Neural Networks\"\n",
    "# by Jason Brownlee\n",
    "# https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n",
    "\n",
    "layer_name = 'conv1'\n",
    "# retrieve weights from the hidden layer\n",
    "filters = getattr(model, layer_name).weight.data.cpu().numpy()\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "n_filters, ix = filters.shape[0], 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[i, :, :, :]\n",
    "    # plot each channel separately\n",
    "    for j in range(filters.shape[1]):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, filters.shape[1], ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[j, :, :], cmap='gray')\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfaqU923uhzm"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3odfN_Uxb2_5"
   },
   "source": [
    "\"Transfer learning is a machine learning method where a model developed for a task is **reused** as the starting point for a model on a second task.\n",
    "\n",
    "It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems.\"\n",
    "\n",
    "Source: [A Gentle Introduction to Transfer Learning for Deep Learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n",
    "\n",
    "So, let's try using a **pre-trained model** as starting point for classifying MNIST's digits!\n",
    "\n",
    "Let's take one the most **famous** neural network architectures for a spin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMO04TrFzVgP"
   },
   "source": [
    "### AlexNet\n",
    "\n",
    "\"AlexNet was the winning entry in ILSVRC 2012. It solves the problem of image classification where the input is an image of one of 1000 different classes (e.g. cats, dogs etc.) and the output is a vector of 1000 numbers.\"\n",
    "\n",
    "To learn more about AlexNet, check out the post where this excerpt of text comes from: [Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZOWs23X23fM"
   },
   "source": [
    "![alt text](https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png)\n",
    "\n",
    "Source: [Learn OpenCV](https://www.learnopencv.com/understanding-alexnet/)\n",
    "\n",
    "It is actually a bit **too much** to use AlexNet for the purpose of classifying MNIST's digits... but let's go for it anyway!\n",
    "\n",
    "How do we use a **pre-trained model**? We first need to **download its weights** from somewhere and then **load them into the corresponding empty model architecture**.\n",
    "\n",
    "To be honest, PyTorch provides a direct way of directly downloading the weights of AlexNet... but let's make it the **hard way**, downloading it manually and only then **loading them into the model**, so we learn how to actually load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BC9SujQl5U_e"
   },
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLFX3x2hq1pP"
   },
   "source": [
    "Let's start by downloading the pre-trained weights... AlexNet has about 230 Mb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth --output alexnet-owt-4df8aa71.pth --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9rfTwvTq-7B"
   },
   "source": [
    "Now, let's create the **empty** network architecture... we could build it from scratch based on the figure above, but it seems too much of a hassle - let's use TorchVision's [**models**](https://pytorch.org/docs/stable/torchvision/models.html#torchvision-models) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "lLSyTfHPulGq",
    "outputId": "97497cbe-4306-4ea3-b424-7b67ccd2d982"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "\n",
    "alex = alexnet()\n",
    "\n",
    "print(alex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mT49YuAPrZF5"
   },
   "source": [
    "First, we load the file into a **state_dict** (remember those?!) using [**torch.load()**](https://pytorch.org/docs/stable/torch.html#torch.load):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "lOBHwRTy4rDW",
    "outputId": "abc94663-9555-410f-b78d-f6d5faff5bf4"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('alexnet-owt-4df8aa71.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3A22QjFRsO7t"
   },
   "source": [
    "Then we load the **state_dict** into the **empty architecture** we created. For this step, we use, obviously, the model's [**load_state_dict()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.load_state_dict) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aWKEHBpKbial",
    "outputId": "c9af40fb-cd40-4a75-cabe-32bb4236f600"
   },
   "outputs": [],
   "source": [
    "alex.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EuuDCSastli"
   },
   "source": [
    "### Freezing Layers\n",
    "\n",
    "OK, we have AlexNet locked and loaded! Should we just **start training it?** \n",
    "\n",
    "Well, not so fast... it has 60 million parameters! Besides, **what would be the point of transfer learning then**?\n",
    "\n",
    "So, we are **freezing its layers**, meaning, we **do not want to update its weights**. How do we do that? **Turning gradients OFF!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwKGmOBqu3Pn"
   },
   "outputs": [],
   "source": [
    "for parameter in alex.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0zkP3edgyJeU"
   },
   "source": [
    "\n",
    "We also need to make a small change... AlexNet was trained to classify **1000 classes**, but we have **only 10 digits**, so we need to **replace the last layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "MDr3cWkFuuKe",
    "outputId": "625710b3-e6f5-41ed-a7f1-4d414a2296b8"
   },
   "outputs": [],
   "source": [
    "print(alex.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJ6Gy3hou5Ww"
   },
   "outputs": [],
   "source": [
    "alex.classifier[6] = nn.Linear(4096, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWQylOXkyqik"
   },
   "source": [
    "Let's double check if the **only trainable parameters** are the ones from our newly created last layer: **classifier.6**.\n",
    "\n",
    "For this, the model's [**named_parameters()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.named_parameters) method come in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_1y3dPvvu-57",
    "outputId": "e4ad9343-7879-468c-f998-52db1fdea37b"
   },
   "outputs": [],
   "source": [
    "for name,param in alex.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJrFzQJizbjo"
   },
   "source": [
    "### Transforming Datasets (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkT_jWJ_zSHP"
   },
   "source": [
    "AlexNet also expects **different inputs**: 3-channel 224 x 224 images.\n",
    "But our **MNIST** data has single-channel 28 x 28 images. So, we can resort to **transforms** once again to transform the latter into the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glayfovRzRi0"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Grayscale, ToTensor, Normalize\n",
    "\n",
    "alex_transforms = Compose([Resize(224),\n",
    "                           Grayscale(num_output_channels=3),\n",
    "                           ToTensor(),\n",
    "                           Normalize((.5, .5, .5), (.5, .5, .5)),])\n",
    "\n",
    "mnist_train = datasets.MNIST('./mnist', train=True, download=True, transform=alex_transforms)\n",
    "mnist_test = datasets.MNIST('./mnist', train=False, download=True, transform=alex_transforms)\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(mnist_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-vu1GAwzhnm"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKmIbw3-B025"
   },
   "source": [
    "Don't forget to **send Alex to the device** :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hDjKdkBvHDa"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "model = alex.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3m-_6OQzknP"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y92ssOobCApd"
   },
   "source": [
    "Nothing new here, but Alex is big, so let's start with 2 epochs only..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xfJiG-dKvOMn",
    "outputId": "5802669a-28fb-4a9e-8b29-21d1401f0bd3"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "losses, val_losses = train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "B2ORqatAwcMI",
    "outputId": "7b8dc5ba-cae8-4d63-953b-2abcfb24ef13"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "89KshvVMwfA0",
    "outputId": "01f62926-b421-4f95-bbde-5162b02d73a5"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_val, y_val in val_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        yhat = model(x_val)\n",
    "        \n",
    "        # this is PyTorch's version of argmax, but it returns a tuple: (max value, index of max value)\n",
    "        _, predicted = torch.max(yhat, 1)\n",
    "        # we get the size of the batch and add up to the total number of samples\n",
    "        total += y_val.size(0)\n",
    "        # we add how many samples got classified correctly\n",
    "        correct += (predicted == y_val).sum().item()\n",
    "        \n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QI7zQQAh1H4u"
   },
   "source": [
    "Doesn't look so impressive now, does it? Anyway..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_9BpAKx5ax7"
   },
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4uddc_t0UZ8"
   },
   "source": [
    "As you can see, a **bigger** model like AlexNet takes **quite some time for training each epoch**, even with all but one of its layers **frozen**.\n",
    "\n",
    "So, it is important to be able to **checkpoint** our model, in case we'd like to **restart training later** (if you get disconnected from Google Colab, for instance!).\n",
    "\n",
    "To checkpoint a model, we basically have to **save its state** into a file, to **load** it back later - nothing special, actually.\n",
    "\n",
    "What defines the **state of a model**?\n",
    "- **model.state_dict()**: kinda obvious, right?\n",
    "- **optimizer.state_dict()**: remember optimizers had the state_dict as well?\n",
    "- **loss**: after all, you should keep track of its evolution\n",
    "- **epoch**: it is just a number, so why not? :-)\n",
    "- **anything else you'd like to have restored**\n",
    "\n",
    "Then, **wrap everything into a Python dictionary** and use [**torch.save()**](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) to dump it all into a file! Easy peasy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQT6KA3g2SRb"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': losses,\n",
    "              'val_loss': val_losses}\n",
    "\n",
    "torch.save(checkpoint, 'alexnet_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FDe2yStg2tLq",
    "outputId": "628b9582-3027-4fcb-ad69-120fd75e1ea7"
   },
   "outputs": [],
   "source": [
    "!ls -l alexnet_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjRDtcIC2slJ"
   },
   "source": [
    "How would you **load** it back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5WccAnA2-Rx"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('alexnet_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "losses = checkpoint['loss']\n",
    "val_losses = checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "9M_DpLoX3FhW",
    "outputId": "a9b1a5c0-757a-4399-b570-62cdeed7b725"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwzjTjc93MgP"
   },
   "source": [
    "Seems about right...\n",
    "\n",
    "You may save a model for **checkpointing**, like we have just done, or for **making predictions**, assuming training is finished.\n",
    "\n",
    "After loading the model, **DO NOT FORGET**:\n",
    "\n",
    "---\n",
    "\n",
    "**SET THE MODE** (not the mood!):\n",
    "- **checkpointing: model.train()**\n",
    "- **predicting: model.eval()**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1JGN8FIgKSE"
   },
   "source": [
    "## Further Improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXy6L02A_idH"
   },
   "source": [
    "Is there **anything else** we can improve or change? Sure, there is **always something else** to add to your model — using a [**learning rate scheduler**](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) or a [**learning rate finder**](https://github.com/davidtvs/pytorch-lr-finder).\n",
    "\n",
    "But this tutorial is already waaaaay too long, so I will stop right here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7v_kLDpO_1kn"
   },
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6_ZPJMa_5DI"
   },
   "source": [
    "I believe this tutorial has **most of the necessary steps** one needs go to trough in order to **learn**, in a **structured** and **incremental** way, how to **develop Deep Learning models using PyTorch**.\n",
    "\n",
    "Hopefully, after finishing working through all code in this post, you’ll be able to better appreciate and more easily work your way through PyTorch’s official [tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "If you have any thoughts, comments or questions, please leave a comment below or contact me on [LinkedIn](https://br.linkedin.com/in/dvgodoy) or [Twitter](https://twitter.com/dvgodoy)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch 101.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
