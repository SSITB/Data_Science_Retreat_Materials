{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Step 1 - Verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "customSchema = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Survived\", DoubleType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True),\n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "customSchema2 = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                           StructField(\"Pclass\", IntegerType(), True),\n",
    "                           StructField(\"Name\", StringType(), True),\n",
    "                           StructField(\"Sex\", StringType(), True),\n",
    "                           StructField(\"Age\", DoubleType(), True),\n",
    "                           StructField(\"SibSp\", IntegerType(), True),\n",
    "                           StructField(\"Parch\", IntegerType(), True),\n",
    "                           StructField(\"Ticket\", StringType(), True),\n",
    "                           StructField(\"Fare\", DoubleType(), True),\n",
    "                           StructField(\"Cabin\", StringType(), True),\n",
    "                           StructField(\"Embarked\", StringType(), True)])\n",
    "\n",
    "train = sqlc.read.csv(\"train.csv\", header=True, schema=customSchema)\n",
    "test = sqlc.read.csv(\"test.csv\", header=True, schema=customSchema2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Step 1 - Short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customSchema = 'PassengerId int, Survived double, Pclass int, Name string, Sex string, Age float, SibSp int, Parch int, Ticket string, Fare float, Cabin string, Embarked string'\n",
    "train = sqlc.read.csv(\"train.csv\", header=True, schema=customSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating summary statistics and turning it into Pandas DF\n",
    "train_desc = train.describe().toPandas().set_index('summary')\n",
    "\n",
    "# Computing correlations between Survived and some features\n",
    "print({col:train.stat.corr('Survived',col) for col in ['Pclass','Age','SibSp','Parch','Fare']})\n",
    "\n",
    "# Checking which columns have NULL values\n",
    "print({col:train.where(train[col].isNull()).count() for col in train.columns})\n",
    "\n",
    "# Taking the mean age from the Pandas DF\n",
    "ageMean = float(train_desc.loc['mean']['Age'])\n",
    "print(ageMean)\n",
    "\n",
    "embarked_mode = train.groupby('Embarked').count().orderBy('count', ascending=False).take(1)[0].Embarked\n",
    "print(embarked_mode)\n",
    "\n",
    "# Filling the Age in both train and test datasets\n",
    "trainFilled = train.na.fill({'Age': ageMean, 'Embarked': embarked_mode})\n",
    "testFilled = test.na.fill({'Age': ageMean, 'Embarked': embarked_mode})\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "train.groupby('Sex','PClass').agg(F.mean('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Applying Estimators and Transformators\n",
    "# Here, I actually fitted and transformed them on the training data\n",
    "# with the purpose of being able to check the intermediate steps\n",
    "\n",
    "indexer1 = (StringIndexer()\n",
    "           .setInputCol(\"Embarked\")\n",
    "           .setOutputCol(\"nEmbarked\")\n",
    "           .setHandleInvalid('skip'))\n",
    "\n",
    "indexed1 = indexer1.fit(trainFilled).transform(trainFilled)\n",
    "\n",
    "indexer2 = (StringIndexer()\n",
    "           .setInputCol(\"Sex\")\n",
    "           .setOutputCol(\"nSex\")\n",
    "           .setHandleInvalid('skip'))\n",
    "\n",
    "indexed2 = indexer2.fit(indexed1).transform(indexed1)\n",
    "\n",
    "encoder1 = OneHotEncoder().setInputCol(\"nEmbarked\").setOutputCol(\"vEmbarked\")\n",
    "encoded1 = encoder1.transform(indexed2)\n",
    "\n",
    "# Using a VectorAssembler to put together all feature columns\n",
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    "                                       'Age',\n",
    "                                       'SibSp',\n",
    "                                       'Parch',\n",
    "                                       'Fare',\n",
    "                                       'nSex',\n",
    "                                       'vEmbarked'],\n",
    "                            outputCol='vFeatures')\n",
    "\n",
    "assembled = assembler.transform(encoded1)\n",
    "\n",
    "# Keeping only the features and label columns to\n",
    "assembled2 = assembled.select(\"Survived\",\"vFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 with R Formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rformula = RFormula(formula='Survived ~ Pclass + Age + SibSp + Parch + Fare + Sex + Embarked',\n",
    "                    featuresCol='vFeatures', labelCol='Survived')\n",
    "rmodel = rformula.fit(trainFilled)\n",
    "features = rmodel.transform(trainFilled)\n",
    "features.select('vFeatures', 'Survived').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = (StandardScaler()\n",
    "          .setInputCol(\"vFeatures\")\n",
    "          .setOutputCol(\"scaledFeat\")\n",
    "          .setWithStd(True)\n",
    "          .setWithMean(True))\n",
    "\n",
    "scalerModel = scaler.fit(assembled2)\n",
    "scaled = scalerModel.transform(assembled2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer1,\n",
    "                            indexer2,\n",
    "                            encoder1,\n",
    "                            assembler,\n",
    "                            scaler])\n",
    "\n",
    "pipeline = Pipeline(stages=[rformula, scaler])\n",
    "\n",
    "model = pipeline.fit(trainFilled)\n",
    "scaled = model.transform(trainFilled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, LogisticRegressionModel\n",
    "\n",
    "# Trains a RF classifier and make predictions\n",
    "rfC = RandomForestClassifier(labelCol='Survived',\n",
    "                        featuresCol='scaledFeat',\n",
    "                        numTrees=50)\n",
    "\n",
    "model = rfC.fit(scaled)\n",
    "\n",
    "predictions = model.transform(scaled)\n",
    "\n",
    "lr = LogisticRegression(labelCol='Survived',\n",
    "                        featuresCol='scaledFeat',\n",
    "                        rawPredictionCol='lr_raw',\n",
    "                        probabilityCol='lr_prob',\n",
    "                        predictionCol='lr_pred')\n",
    "\n",
    "pipeline2 = Pipeline(stages=[indexer1,\n",
    "                            indexer2,\n",
    "                            encoder1,\n",
    "                            assembler,\n",
    "                            scaler,\n",
    "                            rfC,\n",
    "                            lr])\n",
    "\n",
    "model2 = pipeline2.fit(trainFilled)\n",
    "predictions = model2.transform(trainFilled)#.limit(5).toPandas()\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Defines an evaluator based on the metric areaUnderROC\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "# Evaluate the predictions\n",
    "roc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(roc)\n",
    "\n",
    "ev2 = (MulticlassClassificationEvaluator()\n",
    "       .setLabelCol('Survived')\n",
    "       .setPredictionCol('prediction')\n",
    "       .setMetricName('accuracy'))\n",
    "\n",
    "acc = ev2.evaluate(predictions)\n",
    "print(acc)\n",
    "\n",
    "lrmodel = model2.stages[-1]\n",
    "summary = lrmodel.summary\n",
    "summary.accuracy\n",
    "summary.areaUnderROC\n",
    "\n",
    "rfmodel = model2.stages[-2]\n",
    "rfmodel.featureImportances\n",
    "print(rfmodel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 using HandySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from handyspark.extensions import BinaryClassificationMetrics\n",
    "\n",
    "bcm = BinaryClassificationMetrics(scoreAndLabels=predictions,\n",
    "                                  scoreCol='probability',\n",
    "                                  labelCol='Survived')\n",
    "\n",
    "bcm.print_confusion_matrix(.7)\n",
    "bcm.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 filling missing fare values with user-defined function and by mean class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fares_by_class = trainFilled.groupby('Pclass').mean('Fare').toPandas()\n",
    "# Spark hates numpy - make sure we have a list of plain Python floats\n",
    "fares_by_class = list(fares_by_class.sort_values(by='Pclass',ascending=True).values[:,1])\n",
    "fares_by_class = [ float(x) for x in fares_by_class]\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "@udf('float')\n",
    "def fillFare(pclass, fare):\n",
    "    if(fare is None):\n",
    "        return fares_by_class[pclass-1]\n",
    "    else:\n",
    "        return fare\n",
    "\n",
    "testFilled = testFilled.withColumn('Fare', fillFare('Pclass', 'Fare'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the test set a \"table\"\n",
    "testFilled.createOrReplaceTempView('test')\n",
    "\n",
    "# Runs a series of SQL queries to get the number of null values in the test set\n",
    "print({col: sqlc.sql(\"select * from test where \" + col + \" is null\").count() for col in testFilled.columns})\n",
    "\n",
    "# So, there is one null Fare, let's check it\n",
    "sqlc.sql(\"select * from test where Fare is null\").toPandas()\n",
    "\n",
    "# Since the Fare is highly dependent on the class, it makes more sense to use the average for the given class\n",
    "# But we need to take the average from the TRAINING set\n",
    "trainFilled.createOrReplaceTempView('train')\n",
    "avgFare = sqlc.sql(\"select mean(Fare) from train where Pclass = 3\").take(1)[0][0]\n",
    "print(avgFare)\n",
    "\n",
    "# Fill the missing value with the calculated average\n",
    "testFilled = testFilled.na.fill({'Fare': avgFare})\n",
    "\n",
    "predictions_test = model2.transform(testFilled)\n",
    "predictions_test.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = sqlc.read.csv('titanic_answers.csv', header=True)\n",
    "answers = answers.select('PassengerId',F.col('Survived').cast('Double'))\n",
    "\n",
    "pred_answer = predictions_test.join(answers, on='PassengerId')\n",
    "\n",
    "roc = evaluator.evaluate(pred_answer)\n",
    "print(roc)\n",
    "acc = ev2.evaluate(pred_answer)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
